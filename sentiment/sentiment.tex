\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\begin{document}

\begin{center}
{\Large CS221 Fall 2018 Homework 2}

\begin{tabular}{rl}
SUNet ID: & 05794739 \\
Name: & Luis Perez \\
Collaborators: &
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}

\begin{enumerate}[label=(\alph*)]
  \item We first need to know the $\nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w})$. We have:
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w}) &= 0 \tag{$1 - \bm{w}\cdot\phi(x)y \leq 0$} \\
    \nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w}) &= - \phi(x)y
  \end{align*}
  We see then that $\bm{w}$ changes only when $\bm{w}\cdot\phi(x)y < 1$ such that the count for the word in that review either goes up by $\eta$ (for a positive review) or down by $\eta$ (for a negative review) -- this is due to use subtracting the gradient, which has a scaling factor of $-\eta y$ on the feature vector. 

  We start with $w = [0, 0, 0, 0, 0, 0]$ where the features are \{pretty, bad, good, plot, not, scenery \}. On the first update, we note that $\bm{w}\cdot\phi(x)y = 0 < 1$, so we now have $w = [ -0.5, -0.5, 0, 0, 0, 0]$.

  For the second review, we now have $\bm{w}\cdot\phi(x)y = 0 < 1$, so we update to have $w = [-0.5, -0.5, 0.5, 0.5, 0, 0]$.

  On the third review, we have $\bm{w}\cdot\phi(x)y = -0.5 < 1$, so we update to have $w = [-0.5, -0.5, 0, 0.5, -0.5, 0]$.

  On the fourth review, we have $\bm{w}\cdot\phi(x)y = -0.5 < 1$, so we update to have $w = [0.0, -0.5, 0.0, 0.5, -0.5, 0.5]$.

  \item The data set we can use is as follows:
  \begin{itemize}
    \item (-1) not, $\phi(x_1) = [1, 0, 0]^T$
    \item (+1) good, $\phi(x_2) = [0, 1, 0]^T$
    \item (-1) bad, $\phi(x_2) = [0, 0, 1]^T$
    \item (+1) not bad, $\phi(x_3) = [1, 0, 1]^T$
  \end{itemize}
  The consider any linear classifier with weight vector $\bm{w} = [w_1, w_2, w_3]$ where we have $\hat{y}(x) = \bm{w}\cdot\phi(x)$ (we predict +1 if non-negative and -1 if negative). Suppose a linear classifier exists which can correctly classify the data above. Then we must have the below be true, assuming correct classification:
  \begin{align*}
    \bm{w}\phi(x_1) &= w_1 < 0 \\
    \bm{w}\phi(x_2) &= w_2 \geq 0 \\
    \bm{w}\phi(x_3) &= w_3 < 0 \\
    \bm{w}\phi(x_4) &= w_1 + w_3 \geq 0
  \end{align*}
  This is a contradiction, as we can't have $w_1 <0 , w_3 < 0$ and $w_1 + w_3 \geq 0$ (two negative values can't add to a non-negative value). Therefore, the above dataset is not linearly-seperable and a linear classifier cannot possible achieve zero loss.

  To fix the problem, we could add an additional feature which is $0$ for reviews with one word and $1$ for reviews with two words.
  \begin{itemize}
    \item (-1) not, $\phi(x_1) = [1, 0, 0, 0]^T$
    \item (+1) good, $\phi(x_2) = [0, 1, 0, 0]^T$
    \item (-1) bad, $\phi(x_2) = [0, 0, 1, 0]^T$
    \item (+1) not bad, $\phi(x_3) = [1, 0, 1, 1]^T$
  \end{itemize}
  Then note that the weight vector $w = [-1, 1, -1, 3]$ on a linear classifier $w\phi(x)$ will now correctly classify the items in the data set.
  \begin{align*}
    \bm{w}\phi(x_1) &= -1 \\
    \bm{w}\phi(x_2) &= +1 \\
    \bm{w}\phi(x_3) &= -1 \\
    \bm{w}\phi(x_4) &= +1
  \end{align*}
\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[label=(\alph*)]
  \item As described, we have the following loss:
  \begin{align*}
    \text{Loss}(x,y,\bm{w}) &= (\sigma(\bm{w}\cdot\phi(x)) - y)^2 \\
    &= \left(y - \frac{1}{1 + e^{-\bm{w}\dot\phi(x)}}\right)^2
  \end{align*}
  \item We can take the gradient directly, letting $p = \sigma(\bm{w}\cdot\phi(x))$
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}(x,y,\bm{w}) &= -2(y-p) \nabla_w\sigma(\bm{w}\cdot\phi(x)) \tag{chain rule}\\
    &= -2(y-p) p (1 - p)\phi(x)  \tag{derivative of sigmoid as detailed \href{https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x}{here}}\\
  \end{align*}

  \item Suppose we have some arbitrary $\phi(x)$ and $y = 1$. Then we can simplify the gradient expression slightly. 
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}(x,1, \bm{w}) &= -2(1-p)^2 p\phi(x) \\
    &= -2[p - 2p^2 + p^3]\phi(x)
  \end{align*}
  We can make the above arbitrarily small by taking $||\bm{w}|| \to \infty$ with the additionally restriction that $\bm{w} \cdot \phi(x) \neq 0$. To see why, let us see how $p$ is affected as $||\bm{w}||$ changes.
  \begin{align*}
    \lim_{|| w|| \to \infty} p &= \lim_{|| w|| \to \infty} \frac{1}{1 - e^{-\bm{w}\cdot \phi(x)}} \\
    &= \lim_{|| w|| \to \infty} \frac{1}{1 - e^{-||w||\bm{u}\cdot\phi(x)} } \tag{$\bm{u} = \frac{\bm{w}}{||w||}$}
  \end{align*}
  At this point, we have two options. The first, is $\bm{u} \cdot \phi(x) > 0$ or $\bm{u} \cdot \phi(x) < 0$ (the $= 0$ case is not possible by our constraints). In the first case, we'll have:
  $$
    \lim_{|| w|| \to \infty} p = 1
  $$
  while in the second case, we have:
  $$
    \lim_{|| w|| \to \infty} p = 0
  $$
  In either scenario, we have:
  $$
    \lim_{||w || \to \infty} \nabla_{\bm{w}}\text{Loss}(x,y,-c\frac{\phi(x)}{||\phi(x)||_2^2}) = 0
  $$
  From the above, we see that we can make the gradient be as small as we'd like. The intuition is that we can make the gradient arbitrarily small as long as we can make $||\bm{w}||$ arbitrarily large.

  However, we note that the magnitutde of the gradient will never be exactly zero. 

  \item In terms of making the gradient be large, this is achieved when $p - 2p^2 + p^3$ is maximized in the interval $[0,1]$. We note that the derivative is $1 - 4p + 3p^2 = (3p - 1)(p - 1)$ which has roots at $p = 1$ and $p = \frac{1}{3}$. From the results above, we know that $p = 1$ is a local minimum. We note that the function is convex on $[0,1]$, and as such, $p = \frac{1}{3}$ is a local maximum.

  Therefore, maximum magnitude that the gradient can take occurs at $p = \frac{1}{3}$ and is given by:
  $$
    2\left(\frac{4}{27}\right) ||\phi(x) ||_2 = \frac{8}{27}||\phi(x)||_2
  $$

  \item In order to generate our new dataset, we simply transform $y \to y'$ as follows:
  $$
    y' = \ln \left( \frac{y}{1 -y}\right)
  $$

  We claim that there $\exists \bm{w}^*$ such that $\bm{D}'$ has zero loss when the prediction is given by $\hat{y'} = \bm{w}^*\cdot \phi(x)$ (a linear predictor). To see why, first let use solve for $y$ given our above defintion of $y'$:
  \begin{align*}
    y' &= \ln \left( \frac{y}{1- y}\right) \\
    \implies e^{y'} &= \frac{y}{1-y} \\
    \implies e^{y'} &= \frac{1}{\frac{1}{y} - 1} \\
    \implies \frac{1}{y} &= e^{-y'} + 1 \\
    \implies y &= \frac{1}{1 + e^{-y'}}
  \end{align*}
  With the above in hand, let is now see why we can achieve zero loss on our new dataset with standard linear regression. We note that:
  \begin{align*}
    \left(y - \frac{1}{1 + e^{-\bm{w}\cdot \phi(x)}}\right)^2 = 0 \tag{given in the problem that such a $\bm{w}$ exists} \\
    \implies \frac{1}{1 + e^{-y'} } - \frac{1}{1 + e^{-\bm{w}\cdot \phi(x)}} = 0 \tag{solving for $y$ given our definition of $y'$ and substituting} \\
    \implies y' = \bm{w}\cdot \phi(x) \\
    \implies (y' - \bm{w}\cdot \phi(x))^2 = 0
  \end{align*}
  We therefore have $\bm{w}^* = \bm{w}$ which converges to zero loss on $\bm{D}'$.

\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
  \item In ``submissions.py''.
  \item In ``submissions.py''.
  \item In ``submissions.py''.
  \item We look through some incorrect predictions for our system.
  \begin{enumerate}
    \item ``this is a superior horror flick .'' was misclassified as negative (-1) mostly because of the large negative weights associated with ``horror'' and ``flick'' despite the fact that these are mostly noun descriptors for the movie genre, rather than the opinion of the reviewer.
    \item ``you're better off staying home and watching the x-files .'' was misclassified as positive (+1), mostly because it appears to be near the middle -- almost all of the words have low weights ($-0.15 < x <0.23$) and this barely passes as positive.
    \item ``wickedly funny , visually engrossing , never boring , this movie challenges us to think about the ways we consume pop culture .'' was misclassified as negative (-1) almost entirely due to two words (``never'' and ``boring'') which have some of the largest negative weights, despite semantically negating (and becoming positive) in this case.
    \item ``accuracy and realism are terrific , but if your film becomes boring , and your dialogue isn't smart , then you need to use more poetic license .'' was misclassified as positive (+1) due to its complex structure, conditionals, and sheer number of positive to neutral words which become negative only due to the negations and conditionals.
    \item ``a solid film . . . but more conscientious than it is truly stirring .'' was misclassified as positive (+1) mostly due to its use of strongly positive words, such as ``solid'' and ``truly'' which were semantically used as a contrast rather than direct descriptors of the film.
  \end{enumerate}
  \item In ``submissions.py''.
  \item We can see our exploration for different values of $n$ in Table \ref{table:n_values}. The lowest test-error given our tested values is found for $n= 7$, which appears to be a minimum with smaller values increases the test error (insufficient model capacity) and larger values also increasing the test error (overfitting on the training set, or also insufficient training data due to longer n-grams extracting fewer features).

  We can explain the $n \in \{4,5,6,7\}$ nearing and surpassing our performance with the word extractor by nothing a few factors. With a such n-grams, most ``words'' will end-up being partially extracted in their entirety, thereby explaining why a match in performance with our first model is non unexpected. The improvement in performance likely stems from the fact that the n-gram can actually take into account the relationships between words. For example, ``not bad'' will have two word features (``not'' and ``bad'') which will likely both be negative (despite its positive sentiment). However, a 4-gram can capture the near-word feature (``notb'', ``tbad'') as well as the transitional feature sof ``otba'', thereby giving the model the ability to differentiate between just ``not'' (negative) and ``not bad'' (positive).

  We can imagine a review such as the following:

  ``not bad horror flick, never slow, truly marvelous.''
  
  is far more likely to be correctly classified by an n-gram model than a bag-of-words model due to the constant double negations and inter-word relations (``horror flick'') which can only be captured by considering multiple words at once. We can verify this, as our trained bag-of-words models predicts a (-1), mostly due to all the negative words, while our 5-gram model classifies it correctly.

    \begin{table}[h!]
      \centering
      \begin{tabular}{|l|l|l|}
        \hline
        \textbf{}   & \textbf{train error} & \textbf{dev error} \\ \hline
        \textbf{1}  & 0.479                & 0.492              \\ \hline
        \textbf{2}  & 0.317                & 0.410              \\ \hline
        \textbf{3}  & 0.002                & 0.323              \\ \hline
        \textbf{4}  & 0.0                  & 0.281              \\ \hline
        \textbf{5}  & 0.0                  & 0.275              \\ \hline
        \textbf{6}  & 0.0                  & 0.275              \\ \hline
        \textbf{7}  & 0.0                  & 0.273              \\ \hline
        \textbf{8}  & 0.001                & 0.291              \\ \hline
        \textbf{9}  & 0.001                & 0.308              \\ \hline
        \textbf{10} & 0.001                & 0.333              \\ \hline
        \textbf{15} & 0.002                & 0.45               \\ \hline
        \textbf{19} & 0.006                & 0.49               \\ \hline
      \end{tabular}
      \caption{Test and Train Error with n-gram feature extractions}
      \label{table:n_values}
    \end{table}
\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
  \item We run 2-means on the given dataset.
    \begin{enumerate}
      \item We begin the algorithm with the centroids $\mu_1 = [2,3]$ and $\mu_2 = [2,-1]$.
      \begin{enumerate}[label=(\arabic*)]
        \item We first compute the cluster assignments. We have:
        \begin{align*}
          d(\mu_1, \phi(x_1)) &= 10\\
          d(\mu_2, \phi(x_1)) &= 2 \\
          d(\mu_1, \phi(x_2)) &= 2 \\
          d(\mu_2, \phi(x_2)) &= 10 \\
          d(\mu_1, \phi(x_3)) &= 10\\
          d(\mu_2, \phi(x_3)) &= 2 \\
          d(\mu_1, \phi(x_4)) &= 2 \\
          d(\mu_2, \phi(x_4)) &= 9
        \end{align*}
        which implies our assignemnts are:
        \begin{align*}
          z_1 &= 2 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 1
        \end{align*}
      \item Given the above cluster assignments, we can compute the new centroids as:
      \begin{align*}
        \mu_1 &= [1.5, 2] \\
        \mu_2 &= [1.5, 0]
      \end{align*}
      \item We now recompute the assignments, and we have:
      \begin{align*}
        d(\mu_1, \phi(x_1)) &= 4.25\\
        d(\mu_2, \phi(x_1)) &= 2.25 \\
        d(\mu_1, \phi(x_2)) &= 0.25 \\
        d(\mu_2, \phi(x_2)) &= 4.25 \\
        d(\mu_1, \phi(x_3)) &= 8.25 \\
        d(\mu_2, \phi(x_3)) &= 4.25 \\
        d(\mu_1, \phi(x_4)) &= 0.25 \\
        d(\mu_2, \phi(x_4)) &= 4.25
      \end{align*}
      which implies:
      \begin{align*}
        z_1 &= 2 \\
        z_2 &= 1 \\
        z_3 &= 2 \\
        z_4 &= 1
      \end{align*}
      The assignments are unchaged. As such, we can stop the algorithm with the above assignments and centroids.
    \end{enumerate}
    \item We now do another iteration with the initialize centroids as $\mu_1 = [0,1]$ and $\mu_2 = [3,2]$.
    \begin{enumerate}
      \item We first compute the assignments:
        \begin{align*}
          d(\mu_1, \phi(x_1)) &= 2 \\
          d(\mu_2, \phi(x_1)) &= 8 \\
          d(\mu_1, \phi(x_2)) &= 2 \\
          d(\mu_2, \phi(x_2)) &= 4 \\
          d(\mu_1, \phi(x_3)) &= 10 \\
          d(\mu_2, \phi(x_3)) &= 4 \\
          d(\mu_1, \phi(x_4)) &= 5 \\
          d(\mu_2, \phi(x_4)) &= 1
        \end{align*}
        which implies our assignemnts are:
        \begin{align*}
          z_1 &= 1 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 2
        \end{align*}
      \item Given the above cluster assignments, we can compute the new centroids as:
        \begin{align*}
          \mu_1 &= [1, 1] \\
          \mu_2 &= [2.5, 1]
        \end{align*}
      \item We now recompute the assignments, and we have:
        \begin{align*}
          d(\mu_1, \phi(x_1)) &= 1 \\
          d(\mu_2, \phi(x_1)) &= 3.25 \\
          d(\mu_1, \phi(x_2)) &= 1 \\
          d(\mu_2, \phi(x_2)) &= 3.25 \\
          d(\mu_1, \phi(x_3)) &= 5 \\
          d(\mu_2, \phi(x_3)) &= 1.25 \\
          d(\mu_1, \phi(x_4)) &= 2 \\
          d(\mu_2, \phi(x_4)) &= 1.25
        \end{align*}
        which implies:
        \begin{align*}
          z_1 &= 1 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 2
        \end{align*}
         The assignments are unchaged. As such, we can stop the algorithm with the above assignments and centroids.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}

\end{document}