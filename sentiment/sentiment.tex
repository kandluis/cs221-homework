\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\begin{document}

\begin{center}
{\Large CS221 Fall 2018 Homework 2}

\begin{tabular}{rl}
SUNet ID: & 05794739 \\
Name: & Luis Perez \\
Collaborators: &
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}

\begin{enumerate}[label=(\alph*)]
  \item We first need to know the $\nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w})$. We have:
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w}) &= 0 \tag{$1 - \bm{w}\cdot\phi(x)y \leq 0$} \\
    \nabla_{\bm{w}}\text{Loss}_{\text{hinge}}(x, y, \bm{w}) &= - \phi(x)y
  \end{align*}
  We see then that $\bm{w}$ changes only when $\bm{w}\cdot\phi(x)y < 1$ such that the count for the word in that review either goes up by $\eta$ (for a positive review) or down by $\eta$ (for a negative review) -- this is due to use subtracting the gradient, which has a scaling factor of $-\eta y$ on the feature vector. 

  We start with $w = [0, 0, 0, 0, 0, 0]$ where the features are \{pretty, bad, good, plot, not, scenery \}. On the first update, we note that $\bm{w}\cdot\phi(x)y = 0 < 1$, so we now have $w = [ -0.5, -0.5, 0, 0, 0, 0]$.

  For the second review, we now have $\bm{w}\cdot\phi(x)y = 0 < 1$, so we update to have $w = [-0.5, -0.5, 0.5, 0.5, 0, 0]$.

  On the third review, we have $\bm{w}\cdot\phi(x)y = -0.5 < 1$, so we update to have $w = [-0.5, -0.5, 0, 0.5, -0.5, 0]$.

  On the fourth review, we have $\bm{w}\cdot\phi(x)y = -0.5 < 1$, so we update to have $w = [0.0, -0.5, 0.0, 0.5, -0.5, 0.5]$.

  \item The data set we can use is as follows:
  \begin{itemize}
    \item (-1) not, $\phi(x_1) = [1, 0, 0]^T$
    \item (+1) good, $\phi(x_2) = [0, 1, 0]^T$
    \item (-1) bad, $\phi(x_2) = [0, 0, 1]^T$
    \item (+1) not bad, $\phi(x_3) = [1, 0, 1]^T$
  \end{itemize}
  The consider any linear classifier with weight vector $\bm{w} = [w_1, w_2, w_3]$ where we have $\hat{y}(x) = \bm{w}\cdot\phi(x)$ (we predict +1 if non-negative and -1 if negative). Suppose a linear classifier exists which can correctly classify the data above. Then we must have the below be true, assuming correct classification:
  \begin{align*}
    \bm{w}\phi(x_1) &= w_1 < 0 \\
    \bm{w}\phi(x_2) &= w_2 \geq 0 \\
    \bm{w}\phi(x_3) &= w_3 < 0 \\
    \bm{w}\phi(x_4) &= w_1 + w_3 \geq 0
  \end{align*}
  This is a contradiction, as we can't have $w_1 <0 , w_3 < 0$ and $w_1 + w_3 \geq 0$ (two negative values can't add to a non-negative value). Therefore, the above dataset is not linearly-seperable and a linear classifier cannot possible achieve zero loss.

  To fix the problem, we could add an additional feature which is $0$ for reviews with one word and $1$ for reviews with two words.
  \begin{itemize}
    \item (-1) not, $\phi(x_1) = [1, 0, 0, 0]^T$
    \item (+1) good, $\phi(x_2) = [0, 1, 0, 0]^T$
    \item (-1) bad, $\phi(x_2) = [0, 0, 1, 0]^T$
    \item (+1) not bad, $\phi(x_3) = [1, 0, 1, 1]^T$
  \end{itemize}
  Then note that the weight vector $w = [-1, 1, -1, 3]$ on a linear classifier $w\phi(x)$ will now correctly classify the items in the data set.
  \begin{align*}
    \bm{w}\phi(x_1) &= -1 \\
    \bm{w}\phi(x_2) &= +1 \\
    \bm{w}\phi(x_3) &= -1 \\
    \bm{w}\phi(x_4) &= +1
  \end{align*}
\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[label=(\alph*)]
  \item As described, we have the following loss:
  \begin{align*}
    \text{Loss}(x,y,\bm{w}) &= (\sigma(\bm{w}\cdot\phi(x)) - y)^2 \\
    &= \left(y - \frac{1}{1 + e^{-\bm{w}\dot\phi(x)}}\right)^2
  \end{align*}
  \item We can take the gradient directly, letting $p = \sigma(\bm{w}\cdot\phi(x))$
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}(x,y,\bm{w}) &= -2(y-p) \nabla_w\sigma(\bm{w}\cdot\phi(x)) \tag{chain rule}\\
    &= -2(y-p) p (1 - p)\phi(x)  \tag{derivative of sigmoid as detailed \href{https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x}{here}}\\
  \end{align*}

  \item Suppose we have some arbitrary $\phi(x)$ and $y = 1$. Then we can simplify the gradient expression slightly. 
  \begin{align*}
    \nabla_{\bm{w}}\text{Loss}(x,1, \bm{w}) &= -2(1-p)^2 p\phi(x) \\
    &= -2[p - 2p^2 + p^3]\phi(x)
  \end{align*}
  We can make the above arbitrarily small by taking $||\bm{w}|| \to \infty$ with the additionally restriction that $\bm{w} \cdot \phi(x) \neq 0$. To see why, let us see how $p$ is affected as $||\bm{w}||$ changes.
  \begin{align*}
    \lim_{|| w|| \to \infty} p &= \lim_{|| w|| \to \infty} \frac{1}{1 - e^{-\bm{w}\cdot \phi(x)}} \\
    &= \lim_{|| w|| \to \infty} \frac{1}{1 - e^{-||w||\bm{u}\cdot\phi(x)} } \tag{$\bm{u} = \frac{\bm{w}}{||w||}$}
  \end{align*}
  At this point, we have two options. The first, is $\bm{u} \cdot \phi(x) > 0$ or $\bm{u} \cdot \phi(x) < 0$ (the $= 0$ case is not possible by our constraints). In the first case, we'll have:
  $$
    \lim_{|| w|| \to \infty} p = 1
  $$
  while in the second case, we have:
  $$
    \lim_{|| w|| \to \infty} p = 0
  $$
  In either scenario, we have:
  $$
    \lim_{||w || \to \infty} \nabla_{\bm{w}}\text{Loss}(x,y,-c\frac{\phi(x)}{||\phi(x)||_2^2}) = 0
  $$
  From the above, we see that we can make the gradient be as small as we'd like. The intuition is that we can make the gradient arbitrarily small as long as we can make $||\bm{w}||$ arbitrarily large.

  However, we note that the magnitutde of the gradient will never be exactly zero. 

  \item In terms of making the gradient be large, this is achieved when $p - 2p^2 + p^3$ is maximized in the interval $[0,1]$. We note that the derivative is $1 - 4p + 3p^2 = (3p - 1)(p - 1)$ which has roots at $p = 1$ and $p = \frac{1}{3}$. From the results above, we know that $p = 1$ is a local minimum. We note that the function is convex on $[0,1]$, and as such, $p = \frac{1}{3}$ is a local maximum.

  Therefore, maximum magnitude that the gradient can take occurs at $p = \frac{1}{3}$ and is given by:
  $$
    2\left(\frac{4}{27}\right) ||\phi(x) ||_2 = \frac{8}{27}||\phi(x)||_2
  $$

  \item In order to generate our new dataset, we simply transform $y \to y'$ as follows:
  $$
    y' = \ln \left( \frac{y}{1 -y}\right)
  $$

  We claim that there $\exists \bm{w}^*$ such that $\bm{D}'$ has zero loss when the prediction is given by $\hat{y'} = \bm{w}^*\cdot \phi(x)$ (a linear predictor). To see why, first let use solve for $y$ given our above defintion of $y'$:
  \begin{align*}
    y' &= \ln \left( \frac{y}{1- y}\right) \\
    \implies e^{y'} &= \frac{y}{1-y} \\
    \implies e^{y'} &= \frac{1}{\frac{1}{y} - 1} \\
    \implies \frac{1}{y} &= e^{-y'} + 1 \\
    \implies y &= \frac{1}{1 + e^{-y'}}
  \end{align*}
  With the above in hand, let is now see why we can achieve zero loss on our new dataset with standard linear regression. We note that:
  \begin{align*}
    \left(y - \frac{1}{1 + e^{-\bm{w}\cdot \phi(x)}}\right)^2 = 0 \tag{given in the problem that such a $\bm{w}$ exists} \\
    \implies \frac{1}{1 + e^{-y'} } - \frac{1}{1 + e^{-\bm{w}\cdot \phi(x)}} = 0 \tag{solving for $y$ given our definition of $y'$ and substituting} \\
    \implies y' = \bm{w}\cdot \phi(x) \\
    \implies (y' - \bm{w}\cdot \phi(x))^2 = 0
  \end{align*}
  We therefore have $\bm{w}^* = \bm{w}$ which converges to zero loss on $\bm{D}'$.

\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
  \item In ``submissions.py''.
  \item In ``submissions.py''.
  \item In ``submissions.py''.
  \item We look through some incorrect predictions for our system.
  \begin{enumerate}
    \item ``this is a superior horror flick .'' was misclassified as negative (-1) mostly because of the large negative weights associated with ``horror'' and ``flick'' despite the fact that these are mostly noun descriptors for the movie genre, rather than the opinion of the reviewer.
    \item ``you're better off staying home and watching the x-files .'' was misclassified as positive (+1), mostly because it appears to be near the middle -- almost all of the words have low weights ($-0.15 < x <0.23$) and this barely passes as positive.
    \item ``wickedly funny , visually engrossing , never boring , this movie challenges us to think about the ways we consume pop culture .'' was misclassified as negative (-1) almost entirely due to two words (``never'' and ``boring'') which have some of the largest negative weights, despite semantically negating (and becoming positive) in this case.
    \item ``accuracy and realism are terrific , but if your film becomes boring , and your dialogue isn't smart , then you need to use more poetic license .'' was misclassified as positive (+1) due to its complex structure, conditionals, and sheer number of positive to neutral words which become negative only due to the negations and conditionals.
    \item ``a solid film . . . but more conscientious than it is truly stirring .'' was misclassified as positive (+1) mostly due to its use of strongly positive words, such as ``solid'' and ``truly'' which were semantically used as a contrast rather than direct descriptors of the film.
  \end{enumerate}
  \item In ``submissions.py''.
  \item We can see our exploration for different values of $n$ in Table \ref{table:n_values}. The lowest test-error given our tested values is found for $n= 7$, which appears to be a minimum with smaller values increases the test error (insufficient model capacity) and larger values also increasing the test error (overfitting on the training set, or also insufficient training data due to longer n-grams extracting fewer features).

  We can explain the $n \in \{4,5,6,7\}$ nearing and surpassing our performance with the word extractor by nothing a few factors. With a such n-grams, most ``words'' will end-up being partially extracted in their entirety, thereby explaining why a match in performance with our first model is non unexpected. The improvement in performance likely stems from the fact that the n-gram can actually take into account the relationships between words. For example, ``not bad'' will have two word features (``not'' and ``bad'') which will likely both be negative (despite its positive sentiment). However, a 4-gram can capture the near-word feature (``notb'', ``tbad'') as well as the transitional feature sof ``otba'', thereby giving the model the ability to differentiate between just ``not'' (negative) and ``not bad'' (positive).

  We can imagine a review such as the following:

  ``not bad horror flick, never slow, truly marvelous.''
  
  is far more likely to be correctly classified by an n-gram model than a bag-of-words model due to the constant double negations and inter-word relations (``horror flick'') which can only be captured by considering multiple words at once. We can verify this, as our trained bag-of-words models predicts a (-1), mostly due to all the negative words, while our 5-gram model classifies it correctly.

    \begin{table}[h!]
      \centering
      \begin{tabular}{|l|l|l|}
        \hline
        \textbf{}   & \textbf{train error} & \textbf{dev error} \\ \hline
        \textbf{1}  & 0.479                & 0.492              \\ \hline
        \textbf{2}  & 0.317                & 0.410              \\ \hline
        \textbf{3}  & 0.002                & 0.323              \\ \hline
        \textbf{4}  & 0.0                  & 0.281              \\ \hline
        \textbf{5}  & 0.0                  & 0.275              \\ \hline
        \textbf{6}  & 0.0                  & 0.275              \\ \hline
        \textbf{7}  & 0.0                  & 0.273              \\ \hline
        \textbf{8}  & 0.001                & 0.291              \\ \hline
        \textbf{9}  & 0.001                & 0.308              \\ \hline
        \textbf{10} & 0.001                & 0.333              \\ \hline
        \textbf{15} & 0.002                & 0.45               \\ \hline
        \textbf{19} & 0.006                & 0.49               \\ \hline
      \end{tabular}
      \caption{Test and Train Error with n-gram feature extractions}
      \label{table:n_values}
    \end{table}
\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
  \item We run 2-means on the given dataset.
    \begin{enumerate}
      \item We begin the algorithm with the centroids $\bm{\mu}_1 = [2,3]$ and $\bm{\mu}_2 = [2,-1]$.
      \begin{enumerate}[label=(\arabic*)]
        \item We first compute the cluster assignments. We have:
        \begin{align*}
          d(\bm{\mu}_1, \phi(x_1)) &= 10\\
          d(\bm{\mu}_2, \phi(x_1)) &= 2 \\
          d(\bm{\mu}_1, \phi(x_2)) &= 2 \\
          d(\bm{\mu}_2, \phi(x_2)) &= 10 \\
          d(\bm{\mu}_1, \phi(x_3)) &= 10\\
          d(\bm{\mu}_2, \phi(x_3)) &= 2 \\
          d(\bm{\mu}_1, \phi(x_4)) &= 2 \\
          d(\bm{\mu}_2, \phi(x_4)) &= 9
        \end{align*}
        which implies our assignemnts are:
        \begin{align*}
          z_1 &= 2 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 1
        \end{align*}
      \item Given the above cluster assignments, we can compute the new centroids as:
      \begin{align*}
        \bm{\mu}_1 &= [1.5, 2] \\
        \bm{\mu}_2 &= [1.5, 0]
      \end{align*}
      \item We now recompute the assignments, and we have:
      \begin{align*}
        d(\bm{\mu}_1, \phi(x_1)) &= 4.25\\
        d(\bm{\mu}_2, \phi(x_1)) &= 2.25 \\
        d(\bm{\mu}_1, \phi(x_2)) &= 0.25 \\
        d(\bm{\mu}_2, \phi(x_2)) &= 4.25 \\
        d(\bm{\mu}_1, \phi(x_3)) &= 8.25 \\
        d(\bm{\mu}_2, \phi(x_3)) &= 4.25 \\
        d(\bm{\mu}_1, \phi(x_4)) &= 0.25 \\
        d(\bm{\mu}_2, \phi(x_4)) &= 4.25
      \end{align*}
      which implies:
      \begin{align*}
        z_1 &= 2 \\
        z_2 &= 1 \\
        z_3 &= 2 \\
        z_4 &= 1
      \end{align*}
      The assignments are unchaged. As such, we can stop the algorithm with the above assignments and centroids.
    \end{enumerate}
    \item We now do another iteration with the initialize centroids as $\bm{\mu}_1 = [0,1]$ and $\bm{\mu}_2 = [3,2]$.
    \begin{enumerate}
      \item We first compute the assignments:
        \begin{align*}
          d(\bm{\mu}_1, \phi(x_1)) &= 2 \\
          d(\bm{\mu}_2, \phi(x_1)) &= 8 \\
          d(\bm{\mu}_1, \phi(x_2)) &= 2 \\
          d(\bm{\mu}_2, \phi(x_2)) &= 4 \\
          d(\bm{\mu}_1, \phi(x_3)) &= 10 \\
          d(\bm{\mu}_2, \phi(x_3)) &= 4 \\
          d(\bm{\mu}_1, \phi(x_4)) &= 5 \\
          d(\bm{\mu}_2, \phi(x_4)) &= 1
        \end{align*}
        which implies our assignemnts are:
        \begin{align*}
          z_1 &= 1 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 2
        \end{align*}
      \item Given the above cluster assignments, we can compute the new centroids as:
        \begin{align*}
          \bm{\mu}_1 &= [1, 1] \\
          \bm{\mu}_2 &= [2.5, 1]
        \end{align*}
      \item We now recompute the assignments, and we have:
        \begin{align*}
          d(\bm{\mu}_1, \phi(x_1)) &= 1 \\
          d(\bm{\mu}_2, \phi(x_1)) &= 3.25 \\
          d(\bm{\mu}_1, \phi(x_2)) &= 1 \\
          d(\bm{\mu}_2, \phi(x_2)) &= 3.25 \\
          d(\bm{\mu}_1, \phi(x_3)) &= 5 \\
          d(\bm{\mu}_2, \phi(x_3)) &= 1.25 \\
          d(\bm{\mu}_1, \phi(x_4)) &= 2 \\
          d(\bm{\mu}_2, \phi(x_4)) &= 1.25
        \end{align*}
        which implies:
        \begin{align*}
          z_1 &= 1 \\
          z_2 &= 1 \\
          z_3 &= 2 \\
          z_4 &= 2
        \end{align*}
         The assignments are unchaged. As such, we can stop the algorithm with the above assignments and centroids.
      \end{enumerate}
    \end{enumerate}

    \item In ``submission.py''.
    \item We present a modified version of the $K$-means algorithm which incorporates our prior knowledge about how certain examples will cluster. Our inputs to this algorithm are $\{ x_i \}, K$, and $S$ (as defined in the problem statement).
      \begin{enumerate}
        \item The first step in the algorithm is converting $S$ into two more useful data-structures. The first it to take $S$ and convert isa mapping $M = \{i : c_i\}$ such $\forall (i, j) \in S$, $c_i = c_j$. We can do this in $O(n)$ time be iterating over $S$, checking if either $i$ or $j$ is already mapped (or if both, making sure they are consistent) and assigning the corresponding value, or creating a new cluster value. The second data structure is simply the reverse mapping $R = \{c : \{i \mid M[i] = c \} \}$, which can again be done in $O(n)$ time.
        \item The second step is to choose our centroids, $\bm{\mu}_k$ for $k \in \{1, \cdots, K\}$. This can be done in the same way as in the typical $K$-means algorithm. We now enter the training loop.
        \item The first step in this training loop is to compute the assignments $z_i$, leaving $\bm{\mu}_k$ fixed. We break this into two pieces.
          \begin{itemize}
            \item For $i \notin keys(M)$, we compute the assignment the same way we do for $K$-means. That is to say:
              $$
                z_i = \arg\min_{k \in \{1,\cdots,K \}} || \phi(x_i) - \bm{\mu}_k ||_2^2
              $$
            \item For $i \in keys(M)$, we modify the assignment computation to take into account that changing the assignment for $i$ will also change the assignment for all the samples represented by $R[M[i]]$. As such, we must minimize the distance that all of these samples will have to the selected centroid. Note that as an optimization, we can check if we've already already processed the fixed-cluster $c = M[i]$, and if we have, we can skip this example since the assignment won't change for any other elements. As such, what we have is:
              $$
                z_{j : j \in R[M[i]]} = \arg\min_{k \in \{1 \cdots K \}} \sum_{l : l \in R[M[i]]} || \phi(x_{l}) - \bm{\mu}_k ||_2^2
              $$
          \end{itemize}
          With the above, we have now computed all assignments $z_i$ with the additional restrictions imposed by $S$.
        \item The second step in the training loop is to compute the new centers $\bm{\mu}_k$ given that we hold the assignments fixed. We leave this unmodified from the original $K$-means algorithm. That is to say, given our above assignments, we simply compute:
          $$
            \mu_k = \frac{1}{| i : z_i = k|}\sum_{i : z_i = k} \phi(x_i)
          $$
      \end{enumerate}

      The above algorithm is guaranteed to converge to a local minimum, given the constraints. We note that we always satisfy the constraints impossed by $S$, and that our reconstruction loss always decreases at each iteration. For the second step in our iteration loop, this is fairly obvious (as we move the centers to be the average of the selected clusters, which is unmodified from typical $K$-means). For the first step, it takes only a bit of algebra to see that we're decreasing the reconstruction loss. We have:
      \begin{align*}
        \sum_{i=1}^n ||\phi(x_i) - \bm{\mu}_{z_i} ||_2^2 &= \sum_{i \notin keys(M)} || \phi(x_i) - \bm{\mu}_{z_i} ||_2^2 + \sum_{i \in keys(M)} ||\phi(x_i) - \bm{\mu}_{z_i} ||_2^2 \\
        &= \sum_{i \notin keys(M)} || \phi(x_i) - \bm{\mu}_{z_i} ||_2^2  + \sum_{c \in values(M)} \sum_{i \in R[c]} ||\phi(x_i) - \bm{\mu}_{z_i} ||_2^2 \\
      \end{align*}
      As we can see, if we ever elect to change an assignment $z_i$, it will be only because it minimized the reconstruction loss.

    \item By running K-means multiple times on the same dataset, we can better explore the possible results. As was showin in 4a above, K-means simply finds a local-minimum (not the global minimum), and as such, different results are possible. By running K-means multiple times, we can collect a sample of these results and select the one that makes the most sense/has the lowest loss.

    \item If we scale all dimensions of both our initial centroids and our data points by some factor, we are guaranteed to retrieve the same clusters after running $K$-means. This is because the assignment remains unchanged, and the centroids will end-up scaled by the same factor, which implies the assignments on the following iteration will remain unchanged, and so on.

    The assignments are unchanged:
    \begin{align*}
      z_i' &= \arg\min_{k \in \{1, \cdots K\}} || c\phi(x_i) - c\bm{\mu}_k ||_2^2 \\
      &= \arg\min c ||\phi(x_i) - \bm{\mu}_k ||_2^2 \tag{properties of distance metric} \\
      &= \arg\min ||\phi(x_i) - \bm{\mu}_k ||_2^2 \tag{definition of $\arg\min$} \\
      &= z_i
    \end{align*}
    And we note that the new $\bm{\mu}_k'$ are simply $c\bm{\mu}_k$, so the cycle will continue in each iteration.
    \begin{align*}
      \bm{\mu}_k' &= \frac{1}{|\{ i : z_i' = k\}|}\sum_{i:z_i' = k}c \phi(x_i) \\
      &= c \frac{1}{|\{ i : z_i = k\}|}\sum_{i:z_i = k}\phi(x_i)\tag{assignment unchanged, properties of sum} \\
      &= c\bm{\mu}_k
    \end{align*}

    However, scaling just some dimensions does not leave the assignments unchanged. For the extreme example, simply take three points:
    \begin{align*}
      \phi(x_1) &= [0,0] \\
      \phi(x_2) &= [1,0] \\
      \phi(x_3) &= [0,2]
    \end{align*}
    and suppose we initialize our cluster centers for $2$-means as:
    \begin{align*}
      \bm{\mu_1} &= [1,0] \\
      \bm{\mu_2} &= [0,2]
    \end{align*}
    We can immediately see that $z_1 = 1$, $z_2 = 1$ and $z_3 = 2$. However, let's scale the first dimension by some factor, for our purposes, let's pick $c = 10$. We now have:
    \begin{align*}
      \phi(x_1)' &= [0,0] \\
      \phi(x_2)' &= [10,0] \\
      \phi(x_3)' &= [0,2]
    \end{align*}
    and 
     \begin{align*}
      \bm{\mu_1}' &= [10,0] \\
      \bm{\mu_2}' &= [0,2]
    \end{align*}
    Not surprisingly, it is now clear that $z_1' = 2 \neq z_1$ (the other two centers obviously remain unchanged). While trivial, this example demonstrates that scaling along particular directions can have significant effect on the result of $k$-means. The intuition is that the ordering of distances between points is not preserved when only scaling along some dimensions.
  \end{enumerate}

\end{document}